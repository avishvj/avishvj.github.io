<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Avish Vijayaraghavan</title>
    <link>http://avishvj.github.io/</link>
    <description>Recent content on Avish Vijayaraghavan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Mon, 23 Jan 2023 14:12:29 +0000</lastBuildDate><atom:link href="http://avishvj.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Harnessing attention for social justice</title>
      <link>http://avishvj.github.io/posts/tatg/</link>
      <pubDate>Mon, 23 Jan 2023 14:12:29 +0000</pubDate>
      
      <guid>http://avishvj.github.io/posts/tatg/</guid>
      <description>Introduction We’re an internet-based world. Straddling the Millennial-Gen Z divide, I’m part of the last generation to remember what it was like before the internet seeped into every corner of life. And social media is the {cherry on top}{nail in the coffin}, starting as online chatrooms and forums before progressing into more recognisable versions like Twitter and TikTok. It’s the reason we can connect to humans halfway across the globe and the main contributor to Kafkaesque power dynamics that plague much of our society today.</description>
    </item>
    
    <item>
      <title>Reflections on Rudin&#39;s interpretability classic</title>
      <link>http://avishvj.github.io/posts/rudin/</link>
      <pubDate>Fri, 02 Sep 2022 00:28:30 +0100</pubDate>
      
      <guid>http://avishvj.github.io/posts/rudin/</guid>
      <description>Paper Summary In &amp;ldquo;Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead&amp;rdquo;, Prof. Cynthia Rudin argues that explainable machine learning (EML/XAI) and interpretable machine learning (IML) are distinct fields, and that we should prioritise IML models for high-stakes decisions [1]. Rudin defines an IML model as a model which has intrinsic domain-specific constraints for solving certain tasks (e.g. model decomposability into sub-models). XAI is a separate but related field that deals with explaining &amp;ldquo;black box models&amp;rdquo; - models that humans cannot understand directly since they are derived from data without explicit description of how features are combined.</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>http://avishvj.github.io/about/</link>
      <pubDate>Sun, 28 Aug 2022 00:28:30 +0100</pubDate>
      
      <guid>http://avishvj.github.io/about/</guid>
      <description>I&amp;rsquo;m currently studying towards a PhD in AI for Healthcare as part of the third cohort for the AI4Health CDT at Imperial College London. My main PhD project is looking at interpretable, multimodal learning for idiopathic pulmonary fibrosis, supervised by Joram M. Posma, Phil Molyneaux, Tim Ebbels, and Daniel Muthas. Before this, I did a bachelor&amp;rsquo;s in maths and computer science at Imperial and a master&amp;rsquo;s in precision medicine at UCL.</description>
    </item>
    
  </channel>
</rss>
